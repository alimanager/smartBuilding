{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DataFrame does not contain NaN values.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp [dd/mm/yyyy HH:MM]</th>\n",
       "      <th>ki [0:vacant 1:occupied]</th>\n",
       "      <th>o1_1 [0:vacant 1:occupied]</th>\n",
       "      <th>o1_2 [0:vacant 1:occupied]</th>\n",
       "      <th>o1_3 [0:vacant 1:occupied]</th>\n",
       "      <th>o1_4 [0:vacant 1:occupied]</th>\n",
       "      <th>o1_5 [0:vacant 1:occupied]</th>\n",
       "      <th>o2 [0:vacant 1:occupied]</th>\n",
       "      <th>o3 [0:vacant 1:occupied]</th>\n",
       "      <th>o4 [0:vacant 1:occupied]</th>\n",
       "      <th>...</th>\n",
       "      <th>o1_2 [%]</th>\n",
       "      <th>o2 [%]</th>\n",
       "      <th>o3 [%]</th>\n",
       "      <th>o4 [%]</th>\n",
       "      <th>mr [%]</th>\n",
       "      <th>gh [W/m2]</th>\n",
       "      <th>tempOut [C]</th>\n",
       "      <th>rh [%]</th>\n",
       "      <th>wind speed [m/s]</th>\n",
       "      <th>Wind direction [Degree] [North:0 East:90 South:180 West:270]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01/01/2013 00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>23.1</td>\n",
       "      <td>20.8</td>\n",
       "      <td>25.1</td>\n",
       "      <td>22.4</td>\n",
       "      <td>19.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.45</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01/01/2013 00:15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>23.1</td>\n",
       "      <td>20.8</td>\n",
       "      <td>25.1</td>\n",
       "      <td>22.4</td>\n",
       "      <td>19.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.45</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01/01/2013 00:30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>23.1</td>\n",
       "      <td>20.8</td>\n",
       "      <td>25.1</td>\n",
       "      <td>22.4</td>\n",
       "      <td>19.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.45</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01/01/2013 00:45</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>23.1</td>\n",
       "      <td>20.8</td>\n",
       "      <td>25.1</td>\n",
       "      <td>22.4</td>\n",
       "      <td>19.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.45</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01/01/2013 01:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>23.3</td>\n",
       "      <td>20.8</td>\n",
       "      <td>25.1</td>\n",
       "      <td>22.4</td>\n",
       "      <td>20.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>99.0</td>\n",
       "      <td>0.82</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35035</th>\n",
       "      <td>31/12/2013 22:45</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>28.6</td>\n",
       "      <td>27.1</td>\n",
       "      <td>29.6</td>\n",
       "      <td>29.3</td>\n",
       "      <td>28.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>80.0</td>\n",
       "      <td>3.32</td>\n",
       "      <td>118.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35036</th>\n",
       "      <td>31/12/2013 23:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>28.2</td>\n",
       "      <td>27.1</td>\n",
       "      <td>29.7</td>\n",
       "      <td>29.2</td>\n",
       "      <td>28.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>79.0</td>\n",
       "      <td>3.69</td>\n",
       "      <td>124.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35037</th>\n",
       "      <td>31/12/2013 23:15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>28.2</td>\n",
       "      <td>27.1</td>\n",
       "      <td>29.4</td>\n",
       "      <td>29.0</td>\n",
       "      <td>27.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>79.0</td>\n",
       "      <td>3.69</td>\n",
       "      <td>124.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35038</th>\n",
       "      <td>31/12/2013 23:30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>28.7</td>\n",
       "      <td>27.1</td>\n",
       "      <td>29.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>27.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>79.0</td>\n",
       "      <td>3.69</td>\n",
       "      <td>124.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35039</th>\n",
       "      <td>31/12/2013 23:45</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>29.0</td>\n",
       "      <td>26.8</td>\n",
       "      <td>29.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>27.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>79.0</td>\n",
       "      <td>3.69</td>\n",
       "      <td>124.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35040 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      timestamp [dd/mm/yyyy HH:MM]  ki [0:vacant 1:occupied]  \\\n",
       "0                 01/01/2013 00:00                       0.0   \n",
       "1                 01/01/2013 00:15                       0.0   \n",
       "2                 01/01/2013 00:30                       0.0   \n",
       "3                 01/01/2013 00:45                       0.0   \n",
       "4                 01/01/2013 01:00                       0.0   \n",
       "...                            ...                       ...   \n",
       "35035             31/12/2013 22:45                       0.0   \n",
       "35036             31/12/2013 23:00                       0.0   \n",
       "35037             31/12/2013 23:15                       0.0   \n",
       "35038             31/12/2013 23:30                       0.0   \n",
       "35039             31/12/2013 23:45                       0.0   \n",
       "\n",
       "       o1_1 [0:vacant 1:occupied]  o1_2 [0:vacant 1:occupied]  \\\n",
       "0                             0.0                         0.0   \n",
       "1                             0.0                         0.0   \n",
       "2                             0.0                         0.0   \n",
       "3                             0.0                         0.0   \n",
       "4                             0.0                         0.0   \n",
       "...                           ...                         ...   \n",
       "35035                         0.0                         0.0   \n",
       "35036                         0.0                         0.0   \n",
       "35037                         0.0                         0.0   \n",
       "35038                         0.0                         0.0   \n",
       "35039                         0.0                         0.0   \n",
       "\n",
       "       o1_3 [0:vacant 1:occupied]  o1_4 [0:vacant 1:occupied]  \\\n",
       "0                             0.0                         0.0   \n",
       "1                             0.0                         0.0   \n",
       "2                             0.0                         0.0   \n",
       "3                             0.0                         0.0   \n",
       "4                             0.0                         0.0   \n",
       "...                           ...                         ...   \n",
       "35035                         0.0                         0.0   \n",
       "35036                         0.0                         0.0   \n",
       "35037                         0.0                         0.0   \n",
       "35038                         0.0                         0.0   \n",
       "35039                         0.0                         0.0   \n",
       "\n",
       "       o1_5 [0:vacant 1:occupied]  o2 [0:vacant 1:occupied]  \\\n",
       "0                             0.0                       0.0   \n",
       "1                             0.0                       0.0   \n",
       "2                             0.0                       0.0   \n",
       "3                             0.0                       0.0   \n",
       "4                             0.0                       0.0   \n",
       "...                           ...                       ...   \n",
       "35035                         0.0                       0.0   \n",
       "35036                         0.0                       0.0   \n",
       "35037                         0.0                       0.0   \n",
       "35038                         0.0                       0.0   \n",
       "35039                         0.0                       0.0   \n",
       "\n",
       "       o3 [0:vacant 1:occupied]  o4 [0:vacant 1:occupied]  ...  o1_2 [%]  \\\n",
       "0                           0.0                       0.0  ...      23.1   \n",
       "1                           0.0                       0.0  ...      23.1   \n",
       "2                           0.0                       0.0  ...      23.1   \n",
       "3                           0.0                       0.0  ...      23.1   \n",
       "4                           0.0                       0.0  ...      23.3   \n",
       "...                         ...                       ...  ...       ...   \n",
       "35035                       0.0                       0.0  ...      28.6   \n",
       "35036                       0.0                       0.0  ...      28.2   \n",
       "35037                       0.0                       0.0  ...      28.2   \n",
       "35038                       0.0                       0.0  ...      28.7   \n",
       "35039                       0.0                       0.0  ...      29.0   \n",
       "\n",
       "       o2 [%]  o3 [%]  o4 [%]  mr [%]  gh [W/m2]  tempOut [C]  rh [%]  \\\n",
       "0        20.8    25.1    22.4    19.6        2.0          1.6    97.0   \n",
       "1        20.8    25.1    22.4    19.6        2.0          1.6    97.0   \n",
       "2        20.8    25.1    22.4    19.6        2.0          1.6    97.0   \n",
       "3        20.8    25.1    22.4    19.8        2.0          1.6    97.0   \n",
       "4        20.8    25.1    22.4    20.1        2.0          1.2    99.0   \n",
       "...       ...     ...     ...     ...        ...          ...     ...   \n",
       "35035    27.1    29.6    29.3    28.2        1.0          3.4    80.0   \n",
       "35036    27.1    29.7    29.2    28.2        1.0          3.5    79.0   \n",
       "35037    27.1    29.4    29.0    27.9        1.0          3.5    79.0   \n",
       "35038    27.1    29.0    29.0    27.8        1.0          3.5    79.0   \n",
       "35039    26.8    29.0    29.0    27.8        1.0          3.5    79.0   \n",
       "\n",
       "       wind speed [m/s]  \\\n",
       "0                  0.45   \n",
       "1                  0.45   \n",
       "2                  0.45   \n",
       "3                  0.45   \n",
       "4                  0.82   \n",
       "...                 ...   \n",
       "35035              3.32   \n",
       "35036              3.69   \n",
       "35037              3.69   \n",
       "35038              3.69   \n",
       "35039              3.69   \n",
       "\n",
       "       Wind direction [Degree] [North:0 East:90 South:180 West:270]  \n",
       "0                                                   37.0             \n",
       "1                                                   37.0             \n",
       "2                                                   37.0             \n",
       "3                                                   37.0             \n",
       "4                                                   41.0             \n",
       "...                                                  ...             \n",
       "35035                                              118.0             \n",
       "35036                                              124.0             \n",
       "35037                                              124.0             \n",
       "35038                                              124.0             \n",
       "35039                                              124.0             \n",
       "\n",
       "[35040 rows x 64 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Chargement et exploration initiale de la dataset.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import os\n",
    "\n",
    "# Get a list of all .csv files in the current directory\n",
    "csv_files = [file for file in os.listdir('.') if file.endswith('.csv')]\n",
    "\n",
    "# Create a dictionary to store DataFrames with corresponding names for each .csv file\n",
    "dfs = {}\n",
    "\n",
    "# Read each .csv file, rename the DataFrame, and store it in the dictionary\n",
    "for file_name in csv_files:\n",
    "    df_name = file_name.replace('.csv', '')  # Extract DataFrame name from the file name\n",
    "    dfs[df_name] = pd.read_csv(file_name)  # Create DataFrame with the extracted name\n",
    "\n",
    "# Perform the merge based on a common key (e.g., 'common_column')\n",
    "# Replace 'common_column' with the actual column name that is common across all DataFrames\n",
    "merged_df = dfs['01_occ']  # Initialize merged_df with one of the DataFrames\n",
    "for df_name, df in dfs.items():\n",
    "    if df_name != '01_occ':  # Skip the first DataFrame since it's already stored in merged_df\n",
    "        merged_df = pd.merge(merged_df, df, on='timestamp [dd/mm/yyyy HH:MM]',how='outer' )\n",
    "        # merged_df.fillna(method='ffill', inplace=True)\n",
    "        # Forward-fill missing temperature values\n",
    "        merged_df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "\n",
    "# Now you have a merged DataFrame named 'merged_df' containing data from all .csv files\n",
    "nan_df = merged_df.isna()\n",
    "\n",
    "# If there are any NaN values, the nan_df DataFrame will contain True in those positions.\n",
    "# You can check if there are any NaN values in the entire DataFrame by using the any() method.\n",
    "if nan_df.any().any():\n",
    "    print(\"The DataFrame contains NaN values.\")\n",
    "else:\n",
    "    print(\"The DataFrame does not contain NaN values.\")\n",
    "\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['timestamp [dd/mm/yyyy HH:MM]', 'ki [0:vacant 1:occupied]',\n",
       "       'o1_1 [0:vacant 1:occupied]', 'o1_2 [0:vacant 1:occupied]',\n",
       "       'o1_3 [0:vacant 1:occupied]', 'o1_4 [0:vacant 1:occupied]',\n",
       "       'o1_5 [0:vacant 1:occupied]', 'o2 [0:vacant 1:occupied]',\n",
       "       'o3 [0:vacant 1:occupied]', 'o4 [0:vacant 1:occupied]',\n",
       "       'ki  [1:closed 0:open]', 'o1_1 [1:closed 0:open]',\n",
       "       'o1_2 [1:closed 0:open]', 'o1_3 [1:closed 0:open]',\n",
       "       'o1_4 [1:closed 0:open]', 'o2_1 [1:closed 0:open]',\n",
       "       'o2_2 [1:closed 0:open]', 'o3_1 [1:closed 0:open]',\n",
       "       'o3_2 [1:closed 0:open]', 'o3_3 [1:closed 0:open]',\n",
       "       'o3_4 [1:closed 0:open]', 'o4_1 [1:closed 0:open]',\n",
       "       'o4_2 [1:closed 0:open]', 'mr_1 [1:closed 0:open]',\n",
       "       'mr_2 [1:closed 0:open]', 'mr_3 [1:closed 0:open]',\n",
       "       'mr_4 [1:closed 0:open]', 'mr_5 [1:closed 0:open]',\n",
       "       'mr_6 [1:closed 0:open]', 'ki [0:off 1:on]', 'o1_1 [0:off 1:on]',\n",
       "       'o1_2 [0:off 1:on]', 'o2 [0:off 1:on]', 'o3_1 [0:off 1:on]',\n",
       "       'o3_2 [0:off 1:on]', 'o4_1 [0:off 1:on]', 'o4_2 [0:off 1:on]',\n",
       "       'o1_1 [W]', 'o1_2 [W]', 'o1_3 [W]', 'o1_4 [W]', 'o1_5 [W]', 'o2 [W]',\n",
       "       'o3 [W]', 'o4 [W]', 'ki [C]', 'o1_1 [C]', 'o1_2 [C]', 'o2 [C]',\n",
       "       'o3 [C]', 'o4 [C]', 'mr [C]', 'ki [%]', 'o1_1 [%]', 'o1_2 [%]',\n",
       "       'o2 [%]', 'o3 [%]', 'o4 [%]', 'mr [%]', 'gh [W/m2]', 'tempOut [C]',\n",
       "       'rh [%]', 'wind speed [m/s]',\n",
       "       'Wind direction [Degree] [North:0 East:90 South:180 West:270]'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['o4 [0:vacant 1:occupied]', 'o4_1 [1:closed 0:open]',\n",
       "       'o4_2 [1:closed 0:open]', 'o4_1 [0:off 1:on]', 'o4_2 [0:off 1:on]',\n",
       "       'o4 [W]', 'o4 [C]', 'o4 [%]'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.filter(like='o4', axis=1).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    timestamp [dd/mm/yyyy HH:MM]  o4 [0:vacant 1:occupied]  \\\n",
      "timestamp                                                                    \n",
      "2013-01-01 00:00:00             01/01/2013 00:00                       0.0   \n",
      "2013-01-01 00:15:00             01/01/2013 00:15                       0.0   \n",
      "2013-01-01 00:30:00             01/01/2013 00:30                       0.0   \n",
      "2013-01-01 00:45:00             01/01/2013 00:45                       0.0   \n",
      "2013-01-01 01:00:00             01/01/2013 01:00                       0.0   \n",
      "...                                          ...                       ...   \n",
      "2013-12-31 22:45:00             31/12/2013 22:45                       0.0   \n",
      "2013-12-31 23:00:00             31/12/2013 23:00                       0.0   \n",
      "2013-12-31 23:15:00             31/12/2013 23:15                       0.0   \n",
      "2013-12-31 23:30:00             31/12/2013 23:30                       0.0   \n",
      "2013-12-31 23:45:00             31/12/2013 23:45                       0.0   \n",
      "\n",
      "                              timestamp  \n",
      "timestamp                                \n",
      "2013-01-01 00:00:00 2013-01-01 00:00:00  \n",
      "2013-01-01 00:15:00 2013-01-01 00:15:00  \n",
      "2013-01-01 00:30:00 2013-01-01 00:30:00  \n",
      "2013-01-01 00:45:00 2013-01-01 00:45:00  \n",
      "2013-01-01 01:00:00 2013-01-01 01:00:00  \n",
      "...                                 ...  \n",
      "2013-12-31 22:45:00 2013-12-31 22:45:00  \n",
      "2013-12-31 23:00:00 2013-12-31 23:00:00  \n",
      "2013-12-31 23:15:00 2013-12-31 23:15:00  \n",
      "2013-12-31 23:30:00 2013-12-31 23:30:00  \n",
      "2013-12-31 23:45:00 2013-12-31 23:45:00  \n",
      "\n",
      "[35040 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#Data \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import LSTM, Dense, RepeatVector, TimeDistributed\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Charger les données (assurez-vous d'avoir vos données préparées sous forme de DataFrame)\n",
    "# features = ['ki [%]', 'ki [C]', 'gh [W/m2]', 'ki  [1:closed 0:open]', 'tempOut [C]', 'rh [%]','ki [0:off 1:on]', 'wind speed [m/s]']\n",
    "features = ['o4_1 [1:closed 0:open]','o4_1 [0:off 1:on]', 'o4 [W]', 'o4 [C]', 'o4 [%]', 'gh [W/m2]', 'tempOut [C]', 'rh [%]','ki [0:off 1:on]', 'wind speed [m/s]']\n",
    "target = [\"o4 [0:vacant 1:occupied]\"]\n",
    "\n",
    "merged_df = merged_df[['timestamp [dd/mm/yyyy HH:MM]','o4 [0:vacant 1:occupied]']]\n",
    "# Assuming your 'timestamp [dd/mm/yyyy HH:MM]' column is in datetime format\n",
    "merged_df['timestamp'] = pd.to_datetime(merged_df['timestamp [dd/mm/yyyy HH:MM]'])\n",
    "\n",
    "print(merged_df)\n",
    "# # Set the timestamp as the DataFrame index\n",
    "merged_df.set_index('timestamp', inplace=True)\n",
    "# # Creates time series features from datetime index\n",
    "# def create_features(df, label=None):\n",
    "#     # Creates time series features from datetime index\n",
    "#     df['hour'] = df.index.hour\n",
    "#     df['dayofweek'] = df.index.dayofweek\n",
    "#     df['quarter'] = df.index.quarter\n",
    "#     df['month'] = df.index.month\n",
    "#     df['year'] = df.index.year\n",
    "#     df['dayofyear'] = df.index.dayofyear\n",
    "#     df['dayofmonth'] = df.index.day\n",
    "#     df['weekofyear'] = df.index.isocalendar().week\n",
    "    \n",
    "#     X = df[features]\n",
    "   \n",
    "\n",
    "#     if label:\n",
    "#         y = df[label]        \n",
    "#         return X, y\n",
    "#     return X\n",
    "\n",
    "# Filter data based on a specific split date for training & test : \n",
    "# we will take last three month for Testing, \n",
    "# 9 month for Training, 3 for testing \n",
    "\n",
    "split_date = '2013-10-10'\n",
    "train_data = merged_df.loc[merged_df.index <= split_date].copy()\n",
    "test_data = merged_df.loc[merged_df.index > split_date].copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[0. 0. 0. ... 0. 0. 0.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ali\\SmartBuilding\\LSTM Autoencoders.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ali/SmartBuilding/LSTM%20Autoencoders.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m StandardScaler\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ali/SmartBuilding/LSTM%20Autoencoders.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m scaler \u001b[39m=\u001b[39m MinMaxScaler()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ali/SmartBuilding/LSTM%20Autoencoders.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m train_data[\u001b[39m'\u001b[39m\u001b[39mo4 [0:vacant 1:occupied]\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39;49mfit_transform(train_data[\u001b[39m'\u001b[39;49m\u001b[39mo4 [0:vacant 1:occupied]\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ali/SmartBuilding/LSTM%20Autoencoders.ipynb#W3sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m test_data[\u001b[39m'\u001b[39m\u001b[39mo4 [0:vacant 1:occupied]\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39mfit_transform(test_data[\u001b[39m'\u001b[39m\u001b[39mo4 [0:vacant 1:occupied]\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ali/SmartBuilding/LSTM%20Autoencoders.ipynb#W3sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# # Fonction pour créer des séquences temporelles\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ali\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\_set_output.py:142\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    141\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 142\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    143\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    144\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    145\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    146\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    147\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    148\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\ali\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\_set_output.py:142\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    141\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 142\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    143\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    144\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    145\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    146\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    147\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    148\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\ali\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py:848\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    844\u001b[0m \u001b[39m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[0;32m    845\u001b[0m \u001b[39m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[0;32m    846\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    847\u001b[0m     \u001b[39m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m--> 848\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n\u001b[0;32m    849\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    850\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m    851\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\ali\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:427\u001b[0m, in \u001b[0;36mMinMaxScaler.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[39m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    426\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[1;32m--> 427\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpartial_fit(X, y)\n",
      "File \u001b[1;32mc:\\Users\\ali\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:466\u001b[0m, in \u001b[0;36mMinMaxScaler.partial_fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    460\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    461\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMinMaxScaler does not support sparse input. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    462\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConsider using MaxAbsScaler instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    463\u001b[0m     )\n\u001b[0;32m    465\u001b[0m first_pass \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mn_samples_seen_\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 466\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    467\u001b[0m     X,\n\u001b[0;32m    468\u001b[0m     reset\u001b[39m=\u001b[39;49mfirst_pass,\n\u001b[0;32m    469\u001b[0m     dtype\u001b[39m=\u001b[39;49mFLOAT_DTYPES,\n\u001b[0;32m    470\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    471\u001b[0m )\n\u001b[0;32m    473\u001b[0m data_min \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mnanmin(X, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m    474\u001b[0m data_max \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mnanmax(X, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ali\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py:535\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    533\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    534\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 535\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    536\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[0;32m    537\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32mc:\\Users\\ali\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:900\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[39m# If input is 1D raise error\u001b[39;00m\n\u001b[0;32m    899\u001b[0m     \u001b[39mif\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 900\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    901\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mExpected 2D array, got 1D array instead:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39marray=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    902\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    903\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    904\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mif it contains a single sample.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[0;32m    905\u001b[0m         )\n\u001b[0;32m    907\u001b[0m \u001b[39mif\u001b[39;00m dtype_numeric \u001b[39mand\u001b[39;00m array\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mkind \u001b[39min\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mUSV\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    908\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    909\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdtype=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnumeric\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    910\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    911\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[0. 0. 0. ... 0. 0. 0.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "#Train & Test : \n",
    "\n",
    "# X_train, y_train = create_features(train_data, label=target)\n",
    "# X_test, y_test = create_features(test_data, label=target) \n",
    "\n",
    "# Initialize the StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train_data['o4 [0:vacant 1:occupied]'] = scaler.fit_transform(train_data['o4 [0:vacant 1:occupied]'])\n",
    "test_data['o4 [0:vacant 1:occupied]'] = scaler.fit_transform(test_data['o4 [0:vacant 1:occupied]'])\n",
    "\n",
    "\n",
    "# # Fonction pour créer des séquences temporelles\n",
    "def create_sequences(data, sequence_length):\n",
    "    sequences = []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        seq = data.iloc[i:i+sequence_length]\n",
    "        sequences.append(seq.values)\n",
    "    return np.array(sequences)\n",
    "\n",
    "\n",
    "\n",
    "X, y = train_data['o4 [0:vacant 1:occupied]'] , train_data['o4 [0:vacant 1:occupied]'] \n",
    "sequence_length = 10 # Longueur de la séquence temporelle\n",
    "X_train_seq = create_sequences(train_data, sequence_length)\n",
    "X_test_seq = create_sequences(test_data, sequence_length)\n",
    "\n",
    "# X_train_standardized_seq = create_sequences(X_train_standardized, sequence_length)\n",
    "# X_test_standardized_seq = create_sequences(X_test_standardized, sequence_length)\n",
    "\n",
    "# y_train_seq = create_sequences(y_train, sequence_length)\n",
    "# y_test_seq = create_sequences(y_test, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisation\n",
    "\n",
    "# transform of data\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_seq_Norm = scaler.fit_transform(X_train_seq)\n",
    "X_test_seq_Norm = scaler.transform(X_test_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 10, 64)            19200     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 32)                12416     \n",
      "                                                                 \n",
      " repeat_vector (RepeatVecto  (None, 10, 32)            0         \n",
      " r)                                                              \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 10, 32)            8320      \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 10, 64)            24832     \n",
      "                                                                 \n",
      " time_distributed (TimeDist  (None, 10, 1)             65        \n",
      " ributed)                                                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 64833 (253.25 KB)\n",
      "Trainable params: 64833 (253.25 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model: \n",
    "# Build the LSTM autoencoder model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(64, activation='relu', input_shape=(sequence_length, X_train_seq.shape[2]), return_sequences=True))\n",
    "model.add(LSTM(32, activation='relu', return_sequences=False))\n",
    "\n",
    "model.add(RepeatVector(sequence_length))\n",
    "\n",
    "model.add(LSTM(32, activation='relu', return_sequences=True))\n",
    "model.add(LSTM(64, activation='relu', return_sequences=True))\n",
    "\n",
    "model.add(TimeDistributed(Dense(1, activation='sigmoid')))\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type int).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ali\\SmartBuilding\\LSTM Autoencoders.ipynb Cell 7\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ali/SmartBuilding/LSTM%20Autoencoders.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Training the model\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ali/SmartBuilding/LSTM%20Autoencoders.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# history = model.fit(X_train_seq, y_train, epochs=10, batch_size=32, validation_data=(X_test_seq, y_test))\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ali/SmartBuilding/LSTM%20Autoencoders.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# X_train_seq = X_train_seq.astype(np.float32)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ali/SmartBuilding/LSTM%20Autoencoders.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ali/SmartBuilding/LSTM%20Autoencoders.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Training the model\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ali/SmartBuilding/LSTM%20Autoencoders.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(X_train_seq, X_train_seq, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49m(X_test_seq, X_test_seq))\n",
      "File \u001b[1;32mc:\\Users\\ali\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\ali\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:98\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m     96\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[0;32m     97\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 98\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type int)."
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "# history = model.fit(X_train_seq, y_train, epochs=10, batch_size=32, validation_data=(X_test_seq, y_test))\n",
    "# X_train_seq = X_train_seq.astype(np.float32)\n",
    "# y_train = y_train.astype(np.float32)\n",
    "# X_test_seq = X_test_seq.astype(np.float32)\n",
    "# y_test = y_test.astype(np.float32)\n",
    "\n",
    "# Training the model\n",
    "history = model.fit(X_train_seq, X_train_seq, epochs=10, batch_size=32, validation_data=(X_test_seq, X_test_seq))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "y_pred_seq = model.predict(X_test_seq)\n",
    "\n",
    "# Reshape the predictions to match the original shape\n",
    "y_pred = y_pred_seq.reshape(-1, 1)\n",
    "\n",
    "# Rescale the predictions to the original scale\n",
    "# predictions = scaler.inverse_transform(predictions.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test_seq (7957, 10, 16)\n",
      "y_test_seq (7957, 10)\n",
      "10 16\n"
     ]
    }
   ],
   "source": [
    "# Debug\n",
    "print('X_test_seq', X_test_seq.shape)\n",
    "print('y_test_seq', y_test_seq.shape)\n",
    "\n",
    "print(sequence_length, X_train_seq.shape[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error (MSE): {mse}')\n",
    "\n",
    "\n",
    "# mse = mean_squared_error(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
